逻辑回归：本质是分类模型，通过对回归任务设置sigmod函数来减小异常值的影响，然后设置阈值，使输出的结果离散化，从而完成分类任务。
如何求权重向量：极大似然估计，梯度下降，核心思想是令所有样本被预测正确这种情况出现的概率最大
样本预测正确的概率：p(yi|xi)=h(x)^y(1-h(x))^(1-y);h(x)=g(wTx);g(z)=1/1+e^-z)
极大似然函数即为所有样本预测正确的概率的连乘，然后利用梯度下降法，求出使极大似然函数最大的参数。
交叉熵损失函数即为极大似然函数取对数取负。
逻辑回归实现多分类：
1.softmax函数：有相同于类别数的输出，输出的值为样本属于各个类别的概率。
2.建立多个二分类器。
对于类别有明显互斥的使用softmax分类器，有交叉的情况则建立类别个数的逻辑回归分类器。

监督学习和非监督学习
输入的数据有标签则为监督学习，无标签则为非监督学习。

SVM：
定义在特征空间上的间隔最大的线性分类器
核心思想是要样本离分里面尽可能远
硬间隔和软间隔：引入软间隔是为了处理异常值离群点。
核函数：低维数据不是线性可分的，使用核函数将其映射到高维变为线性可分
常用的核函数：线性核，高斯核，sigmod核
数据量较少使用高斯核，数据量较大使用线性核。

生成模式和判别模式：
生成模式：学习得到联合概率分布，然后计算后验概率进行预测。如朴素贝叶斯，混合高斯，马尔科夫随机场
判别模式：学习得到后验概率进行预测。LR,SVM,神经网络，Boosting
